{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlce3TdLJExHKKKtzk0RSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkleitsas/ppo_cartpole/blob/main/PPO_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUyv1T8dZMNb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0003\n",
        "gamma = 0.99\n",
        "lambda_gae = 0.97\n",
        "eps_clip = 0.2\n",
        "training_iterations = 30\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        action_probs = self.softmax(x)\n",
        "        return action_probs\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self):\n",
        "        self.memory = Memory()\n",
        "        self.actor = Actor().to(device)\n",
        "        self.critic = Critic().to(device)\n",
        "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
        "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=learning_rate)\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_probs = self.actor(state)\n",
        "\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action).item()\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        states = torch.stack(self.memory.states, dim=0).to(device)\n",
        "        actions = torch.LongTensor(self.memory.actions).to(device)\n",
        "        rewards = torch.FloatTensor(self.memory.rewards).to(device)\n",
        "        old_logprobs = torch.FloatTensor(self.memory.logprobs).to(device)\n",
        "        dones = torch.FloatTensor(self.memory.dones).to(device)\n",
        "\n",
        "\n",
        "        state_values = self.critic(states)\n",
        "        state_values = torch.cat((state_values.squeeze(), torch.tensor([0.0], device=device)))\n",
        "        state_values = state_values.squeeze().detach()\n",
        "        gaes = torch.zeros_like(rewards).to(device)\n",
        "        returns = torch.zeros_like(rewards).to(device)\n",
        "        advantage = 0\n",
        "\n",
        "\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + gamma * (1 - dones[t]) * state_values[t + 1] - state_values[t]\n",
        "            advantage = delta + gamma * lambda_gae * (1 - dones[t]) * advantage\n",
        "            gaes[t] = advantage\n",
        "            returns[t] = advantage + state_values[t]\n",
        "\n",
        "        for _ in range(training_iterations):\n",
        "            action_probs = self.actor(states)\n",
        "            state_values = self.critic(states)\n",
        "            dist = Categorical(action_probs)\n",
        "            new_logprobs = dist.log_prob(actions)\n",
        "            state_values = state_values.squeeze()\n",
        "\n",
        "            ratios = torch.exp(new_logprobs - old_logprobs.detach())\n",
        "\n",
        "            advantages = gaes\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - eps_clip, 1 + eps_clip) * advantages\n",
        "            loss_actor = -torch.min(surr1, surr2)\n",
        "            loss_critic = self.MseLoss(state_values, returns)\n",
        "\n",
        "            self.actor_opt.zero_grad()\n",
        "            self.critic_opt.zero_grad()\n",
        "            loss_actor.mean().backward()\n",
        "            loss_critic.mean().backward()\n",
        "            self.actor_opt.step()\n",
        "            self.critic_opt.step()\n",
        "\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "    def push(self, state, action, logprob, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.logprobs.append(logprob)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "agent = PPO()\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "rewards_past_x = []\n",
        "consecutive_solves = 0\n",
        "\n",
        "for episode in range(1000):  # Train for 1000 episodes\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Get the action from your PPO model\n",
        "        action, log_prob = agent.select_action(state)  # Replace with your PPO's method\n",
        "\n",
        "        # Step in the environment\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        agent.memory.push(torch.from_numpy(state), action, log_prob, reward, done)\n",
        "        # Optionally: Render the environment (for visualization)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "    rewards_past_x.append(total_reward)\n",
        "    if (episode + 1) % 5 == 0:\n",
        "        agent.update()\n",
        "        agent.memory.clear_memory()\n",
        "    if (episode + 1) % 10 == 0:\n",
        "        average_reward = sum(rewards_past_x) / len(rewards_past_x)\n",
        "        print(f\"Episode {episode + 1}: Average Last 10 Reward = {average_reward}\")\n",
        "        if average_reward == 500.0:\n",
        "            consecutive_solves += 1\n",
        "            if consecutive_solves == 2:\n",
        "                print(\"Cart Pole Solved\\n Terminating...\")\n",
        "                break\n",
        "        else:\n",
        "          consecutive_solves = 0\n",
        "        rewards_past_x = []\n",
        "\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAZa_jusktVg",
        "outputId": "58de7e3e-e161-4e22-e71b-f47910a85d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10: Average Last 10 Reward = 16.3\n",
            "Episode 20: Average Last 10 Reward = 18.7\n",
            "Episode 30: Average Last 10 Reward = 35.2\n",
            "Episode 40: Average Last 10 Reward = 32.7\n",
            "Episode 50: Average Last 10 Reward = 31.1\n",
            "Episode 60: Average Last 10 Reward = 34.2\n",
            "Episode 70: Average Last 10 Reward = 76.5\n",
            "Episode 80: Average Last 10 Reward = 110.9\n",
            "Episode 90: Average Last 10 Reward = 155.7\n",
            "Episode 100: Average Last 10 Reward = 176.0\n",
            "Episode 110: Average Last 10 Reward = 194.4\n",
            "Episode 120: Average Last 10 Reward = 139.1\n",
            "Episode 130: Average Last 10 Reward = 116.9\n",
            "Episode 140: Average Last 10 Reward = 149.5\n",
            "Episode 150: Average Last 10 Reward = 191.3\n",
            "Episode 160: Average Last 10 Reward = 286.1\n",
            "Episode 170: Average Last 10 Reward = 258.0\n",
            "Episode 180: Average Last 10 Reward = 283.7\n",
            "Episode 190: Average Last 10 Reward = 342.6\n",
            "Episode 200: Average Last 10 Reward = 349.2\n",
            "Episode 210: Average Last 10 Reward = 448.1\n",
            "Episode 220: Average Last 10 Reward = 462.9\n",
            "Episode 230: Average Last 10 Reward = 451.2\n",
            "Episode 240: Average Last 10 Reward = 413.6\n",
            "Episode 250: Average Last 10 Reward = 457.2\n",
            "Episode 260: Average Last 10 Reward = 470.6\n",
            "Episode 270: Average Last 10 Reward = 466.9\n",
            "Episode 280: Average Last 10 Reward = 500.0\n",
            "Episode 290: Average Last 10 Reward = 470.5\n",
            "Episode 300: Average Last 10 Reward = 430.1\n",
            "Episode 310: Average Last 10 Reward = 476.4\n",
            "Episode 320: Average Last 10 Reward = 458.8\n",
            "Episode 330: Average Last 10 Reward = 500.0\n",
            "Episode 340: Average Last 10 Reward = 500.0\n",
            "Cart Pole Solved\n",
            " Terminating...\n"
          ]
        }
      ]
    }
  ]
}